# Cache Optimization
$$\text{Average memory access time} = \text{Hit time} + (\text{Miss rate} \times \text{Miss penalty})$$
[[Cache Memory#Fundamentals]]

- Reduce average AMAT

## Larger block sizes

> Reduce miss rate

### Advantages
- Utilize spatial locality
- Reduce compulsory misses

### Disadvantages

- Increases miss penalty
	- Takes more time to fetch a block to cache
		- Can only bring one word at a time
		- Bus width issue
- Increases conflict misses
	- More number of blocks mapped to same location
	- May bring useless data and evict useful data (pollution)


## Larger cache

> Reduce miss rate

### Advantages

- Reduces capacity misses
- Accommodate larger memory footprint

### Disadvantages

- Longer hit time
	- More to index
- Higher cost, area, power

## Higher Associativity

- Fully associative (put anywhere) cache is best, but has high hit time

### Advantages

- Reduce conflict misses
- Reduce miss rate and eviction rate

### Disadvantages

- Increase hit time
- Complex design than direct mapped
- More time to search in the set
	- Tag comparison time


## Multilevel caches

> Reduce miss penalty

- Fast enough to keep pace with processor
- Large to overcome widening gap between processor and memory

1. L1 small enough to match clock cycle time (low hit time)
2. L2 large enough to capture many accesses that would go to main memory thereby lessening effective miss penalty (low miss rate)


$$\text{AMAT} = \text{Hit time}_{L1} + \text{Miss rate}_{L1} \times (\text{Hit time}_{L2} + \text{Miss rate}_{L2} \times \text{Miss penalty}_{L2})$$

- **Local miss rate:** Number of misses in a cache level / number of memory accesses to that level
- **Global miss rate:** Number of misses in a cache level / number of memory access generated by CPU

### Inclusive & exclusive cache

- **Inclusive:** $L1 \subset L2$ 
- **Exclusive:** $L1$ & $L2$ are mutually exclusive

![[inclusive-exclusive-cache.png | 600]]

## Prioritize read miss over write

> Reduces miss penalty

- Whenever we have a write operation and a read operation to be done, prioritize the read

If a read miss has to evict a dirty memory block, the normal sequence is to write dirty block to memory and read missed block
- CPU will be waiting for read miss

**Optimization:** Copy dirty block to buffer, read from memory, then write to block – reduces CPU waiting time on read miss
- Read _then_ write

## Way prediction

- Direct mapped cache have lowest hit time
- Inherit property of direct mapped time in terms of hit time with a 4-way associative cache
- Predict the way to pre-set multiplexer
	- Extra bits set to predict the block within the set
- Remember the MRU of a given set
- Using prediction bits we can _power gating_, where we can temporarily keep units in low power mode

## Victim caches

> Reduce miss penalty

- Additional storage near L1 for MR evicted blocks
- Efficient for thrashing problem in L1 cache
- Lookup in victim cache before L2

- L1 and victim cache exclusive

why not use a larger L1?

## Pipelining cache

>Split cache memory access into substages

- Faster clock cycle time

1. Indexing
2. Tag read
3. Hit/miss check
4. Data transfer

- Increases branch misprediction penalty
- Easier to increase associativity

## Multi-banked cache

> Divide cache into independent banks that support simultaneous access

- Increase cache bandwidth
- Sequential interleaving
- Spatial locality – accessing address 0 means high chance next address accessed will be 1

![[multi-banked-cache.png]]

## Hardware prefetching

- Prefetching to reduce miss rate and miss penalty
	- _Before_ processor requests them
- Fetch more blocks on miss – include next sequential block
- Requested block kept in I-cache and next in stream buffer
- If missed block in stream buffer, cache miss is cancelled