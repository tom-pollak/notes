---
date created: 2021-12-08 17:21

---

# Philosophical Issues

## Godels Incompleteness Theorem

> Proved in first-order logic that any axiomatic logic powerful to model arithmetic is unable to represent all mathematics
>
> - Precursor to [[Rice's Theorem & Church-Turing Thesis#Church-Turing Thesis]]

- Argued that machines are inferior to humans as machines are limited by incompleteness theorem
  - Based on idea that computers are Turing Machines
  - Only applies to formal systems powerful enough to perform arithmetic
  - Therefore argument dosen't stricly apply to computers as we know them
  - Incompleteness not necessarily a bad thing

Theorem effectively shows that types of sentence cannot be proven true:

- “This sentence is false”
- We arguably are also subject to Godels theorem (although hard to show)

## Functionalism

> Anything that forms a function is that thing

- Does it matter if a heart is a muscle or metal?
- Implies that thought and mental processes can be captured within functions
- Sufficiently complex computers should be able to implement those functions, hence think

## Chinese Room Argument

> Argument against functionalism

- Room person in it with books describing how to turn some symbols into other symbols
- Get input in one side, push out the conversion the other side
- While the room seems to understand Chinese, it doesn't

> Look in the room, where is the knowledge?

- Could argue similar about the brain: look in the skull, where is the knowledge
- Is computation taking place in the scenario?
- Where in the room does the computation take place?
- Something must use the room for it to be doing computation?

## Reductionism vs Holism

> **Reductionism:** Everything at a macroscopic level can be described by reducing to a microscopic level
>
> - e.g. illness, intelligence, behaviour can be reduced to changes in genetic information

> **Holism:** Should consider whole system rather than individual components
>
> - Considering the interactions between neurons, rather than neurons themselves
> - A single ant is not intelligent, while the colony is: the ant colony is smarter than the sum of its parts

## Extrospection to Introspection

- How do you know if a computer **understands** something?
- How do you if a human understands something?
- How do you **know** that you understand something?
  - So used to applying technique, that you don't understand something anymore

## Beetle-in-a-Box

- Everybody has a box containing a “beetle”
- They carry this box everywhere, and can look inside the box at any time
- No one can look into anyone else's box, and everybody says they know what a beetle is only by looking at _their_ beetle
- Without a point of reference, no way of knowing that somebody else's box actually contains what your box contains
  - Originally applied to pain

> Only way to infer if another human understands a topic is to observe their behaviour while testing them

- Turing Test
- AI effect
  - “AI is whatever hasn't been done yet”
  - Like deep blue computer, didn't really beat him just brute forced combinations
